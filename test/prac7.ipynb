{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º OurNeuralNetwork."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8151036049051821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: f(x) = 1/(1+e^(-x))\n",
    "    return 1/(1+np.exp(-x))\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "weights = np.array([0,1])    # w1 = 0, w2 = 1\n",
    "bias = 4    # c = 4\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([2,3])   # x = 2, y = 3\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.input    = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y        = y\n",
    "        self.output   = np.zeros(y.shape)\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "class OurNeuralNetwork:\n",
    "    \"\"\"\n",
    "    –¥–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
    "        ‚àí —Ç—Ä–∏ –≤—Ö–æ–¥–∞ (ùë•1,ùë•2,ùë•3);\n",
    "        ‚àí —Ç—Ä–∏ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (‚Ñé1,‚Ñé2,‚Ñé3);\n",
    "        ‚àí –≤—ã—Ö–æ–¥ (ùëú1).\n",
    "    –Ω–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
    "        ‚àí ùë§ = [0.5,0.5,0.5]\n",
    "        ‚àí ùëè = 0\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.5,0.5,0.5])\n",
    "        bias = 0\n",
    "        # –∫–ª–∞—Å—Å Neuron –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.h3 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        # –≤—Ö–æ–¥—ã –¥–ª—è –æ1 - —ç—Ç–æ –≤—ã—Ö–æ–¥—ã h1 –∏ h2 –∏ h3\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2,3,4])\n",
    "print(network.feedforward(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8671195555587996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class OurNeuralNetwork:\n",
    "    \"\"\"\n",
    "    –¥–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
    "        - –¥–≤–∞ –≤—Ö–æ–¥–∞\n",
    "        - –¥–≤–∞ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (h1, h2)\n",
    "        - –≤—ã—Ö–æ–¥ (–æ1, o2)\n",
    "    –Ω–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
    "        - w = [1,0]\n",
    "        - b = 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        weights = np.array([1,0])\n",
    "        bias = 1\n",
    "        # –∫–ª–∞—Å—Å Neuron –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "        self.o2 = Neuron(weights, bias)\n",
    "        self.o = Neuron(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        # –≤—Ö–æ–¥—ã –¥–ª—è –æ1 - —ç—Ç–æ –≤—ã—Ö–æ–¥—ã h1 –∏ h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h2, out_h1]))\n",
    "        out_o = self.o.feedforward(np.array([out_o2, out_o1]))\n",
    "        return out_o\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2,3])\n",
    "print(network.feedforward(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥—Ä—É–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "MLPClassifier\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Sizes:  (1797, 64) (1797,)\n",
      "Dataset Sizes:  (506, 13) (506,)\n",
      "Train/Test sizes:  (1437, 64) (360, 64) (1437,) (360,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sad/PycharmProjects/pythonProject2/venv/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "libtk8.6.so: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [5], line 45\u001B[0m\n\u001B[1;32m     20\u001B[0m MLPClassifier(\n\u001B[1;32m     21\u001B[0m     activation\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mrelu\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     22\u001B[0m     alpha\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.0001\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     42\u001B[0m     warm_start\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     43\u001B[0m )\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m pyplot \u001B[38;5;28;01mas\u001B[39;00m plt\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mturtle\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m color\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m confusion_matrix\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mplot_confusion_matrix\u001B[39m(Y_test, Y_preds):\n",
      "File \u001B[0;32m/usr/lib/python3.10/turtle.py:107\u001B[0m\n\u001B[1;32m    103\u001B[0m _ver \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mturtle 1.1b- - for Python 3.1   -  4. 5. 2009\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;66;03m# print(_ver)\u001B[39;00m\n\u001B[0;32m--> 107\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtkinter\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mTK\u001B[39;00m\n\u001B[1;32m    108\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtypes\u001B[39;00m\n\u001B[1;32m    109\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmath\u001B[39;00m\n",
      "File \u001B[0;32m/usr/lib/python3.10/tkinter/__init__.py:37\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtypes\u001B[39;00m\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01m_tkinter\u001B[39;00m \u001B[38;5;66;03m# If this fails your Python may not be configured for Tk\u001B[39;00m\n\u001B[1;32m     38\u001B[0m TclError \u001B[38;5;241m=\u001B[39m _tkinter\u001B[38;5;241m.\u001B[39mTclError\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtkinter\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n",
      "\u001B[0;31mImportError\u001B[0m: libtk8.6.so: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits, load_boston\n",
    "\n",
    "digits = load_digits()\n",
    "X_digits, Y_digits = digits.data, digits.target\n",
    "print('Dataset Sizes: ', X_digits.shape, Y_digits.shape)\n",
    "from sklearn.datasets import load_digits, load_boston\n",
    "\n",
    "boston = load_boston()\n",
    "X_boston, Y_boston = boston.data, boston.target\n",
    "print('Dataset Sizes: ', X_boston.shape, Y_boston.shape)\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_classifier = MLPClassifier(random_state=123)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_digits, Y_digits,\n",
    "                                     train_size=0.80, test_size=0.20, stratify=Y_digits, random_state=123)\n",
    "\n",
    "print('Train/Test sizes: ', X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "mlp_classifier.fit(X_train, Y_train)\n",
    "MLPClassifier(\n",
    "    activation='relu',\n",
    "    alpha=0.0001,\n",
    "    batch_size='auto',\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    early_stopping=False,\n",
    "    epsilon=1e-08,\n",
    "    hidden_layer_sizes=(100,),\n",
    "    learning_rate='constant',\n",
    "    learning_rate_init=0.001,\n",
    "    max_iter=200,\n",
    "    momentum=0.9,\n",
    "    n_iter_no_change=10,\n",
    "    nesterovs_momentum=True,\n",
    "    power_t=0.5,\n",
    "    random_state=123,\n",
    "    shuffle=True,\n",
    "    solver='adam',\n",
    "    tol=0.0001,\n",
    "    validation_fraction=0.1,\n",
    "    verbose=False,\n",
    "    warm_start=False\n",
    ")\n",
    "from matplotlib import pyplot as plt\n",
    "from turtle import color\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def plot_confusion_matrix(Y_test, Y_preds):\n",
    "    conf_mat = confusion_matrix(Y_test, Y_preds)\n",
    "    #print(conf_mat)\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.matshow(conf_mat, cmap=plt.cm.Blues, fignum=1)\n",
    "    plt.yticks(range(10), range(10))\n",
    "    plt.xticks(range(10), range(10))\n",
    "    plt.colorbar();\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            plt.text(i-0.2,j+0.1, str(conf_mat[j,i]), color='tab:red')\n",
    "plot_confusion_matrix(Y_test, mlp_classifier.predict(X_test))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "–ó–∞–¥–∞–Ω–∏–µ\n",
    "\n",
    "–ó–∞–¥–∞—á–∞: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã MLPClassified –∏ MLPRegressor –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –∞—Ç—Ä–∏–±—É—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–∂–µ—Ç–µ –≤–∑—è—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ò—Ä–∏—Å–æ–≤: https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv –∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –æ—Ç –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã: https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sad/PycharmProjects/pythonProject2/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3554/3458819431.py:22: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  print(y_test[:15])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Setosa' 'Setosa' 'Virginica' 'Versicolor' 'Virginica' 'Versicolor'\n",
      " 'Setosa' 'Setosa' 'Setosa' 'Setosa' 'Setosa' 'Virginica' 'Versicolor'\n",
      " 'Versicolor' 'Versicolor']\n",
      "19         Setosa\n",
      "49         Setosa\n",
      "83     Versicolor\n",
      "93     Versicolor\n",
      "113     Virginica\n",
      "69     Versicolor\n",
      "31         Setosa\n",
      "37         Setosa\n",
      "25         Setosa\n",
      "10         Setosa\n",
      "39         Setosa\n",
      "107     Virginica\n",
      "51     Versicolor\n",
      "94     Versicolor\n",
      "50     Versicolor\n",
      "Name: variety, dtype: object\n",
      "Test Accurancy: 0.933\n",
      "Training Accurancy: 0.975\n",
      "Loss:  0.32402012120284457\n",
      "Number of coefs:  2\n",
      "Number of intercepts:  2\n",
      "Number of iterations for which estimator ran:  200\n",
      "Name of output layer activation function:  softmax\n",
      "(24, 1) (6, 1) (24,) (6,)\n",
      "[20.26234628 55.2781752  18.82135812 50.48274487 20.26234628 50.9622879 ]\n",
      "[ 54445. 121872.  56642. 116969.  64445. 112635.]\n",
      "Test R^2: -8.796\n",
      "Training R^2: -8.261\n",
      "Loss:  2988058032.1601596\n",
      "Number of coefs:  2\n",
      "Number of intercepts:  2\n",
      "Number of iterations for which estimator ran:  200\n",
      "Name of output layer activation function:  identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sad/PycharmProjects/pythonProject2/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = r'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.iloc[:,:-1],    # –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π - –≤ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    data.iloc[:,-1], # –ø–æ—Å–ª–µ–¥–Ω—é—é –≤ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–∫–ª–∞—Å—Å)\n",
    "    test_size = 0.20 # —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ 20%\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_classifier = MLPClassifier(random_state=123)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "Y_preds = mlp_classifier.predict(X_test)\n",
    "\n",
    "print(Y_preds[:15])\n",
    "print(y_test[:15])\n",
    "## –º–º—Ç–µ–¥–æ Score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "print('Test Accurancy: %.3f'%mlp_classifier.score(X_test, y_test))\n",
    "print('Training Accurancy: %.3f'%mlp_classifier.score(X_train, y_train))\n",
    "print(\"Loss: \", mlp_classifier.loss_)\n",
    "print(\"Number of coefs: \", len(mlp_classifier.coefs_))\n",
    "print(\"Number of intercepts: \", len(mlp_classifier.intercepts_))\n",
    "print(\"Number of iterations for which estimator ran: \", mlp_classifier.n_iter_)\n",
    "print(\"Name of output layer activation function: \", mlp_classifier.out_activation_)\n",
    "url = r'https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv'\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.iloc[:,:-1].values,    # –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π - –≤ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    data.iloc[:,-1].values, # –ø–æ—Å–ª–µ–¥–Ω—é—é –≤ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–∫–ª–∞—Å—Å)\n",
    "    test_size = 0.20, # —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ 20%\n",
    "    random_state=123\n",
    ")\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_regressor = MLPRegressor(random_state=123)\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "Y_preds = mlp_regressor.predict(X_test)\n",
    "\n",
    "print(Y_preds[:10])\n",
    "print(y_test[:10])\n",
    "## –º–º—Ç–µ–¥–æ Score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "print('Test R^2: %.3f'%mlp_regressor.score(X_test, y_test))\n",
    "print('Training R^2: %.3f'%mlp_regressor.score(X_train, y_train))\n",
    "print(\"Loss: \", mlp_regressor.loss_)\n",
    "print(\"Number of coefs: \", len(mlp_regressor.coefs_))\n",
    "print(\"Number of intercepts: \", len(mlp_regressor.intercepts_))\n",
    "print(\"Number of iterations for which estimator ran: \", mlp_regressor.n_iter_)\n",
    "print(\"Name of output layer activation function: \", mlp_regressor.out_activation_)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
