{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "–†–µ–∞–ª–∏–∑–æ–≤–∞—Ç—å –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–π –ø–æ –∞–Ω–∞–ª–æ–≥–∏–∏ —Å –∫–ª–∞—Å—Å–æ–º OurNeuralNetwork."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8151036049051821\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def sigmoid(x):\n",
    "    # –§—É–Ω–∫—Ü–∏—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏: f(x) = 1/(1+e^(-x))\n",
    "    return 1/(1+np.exp(-x))\n",
    "class Neuron:\n",
    "    def __init__(self, weights, bias):\n",
    "        self.weights = weights\n",
    "        self.bias = bias\n",
    "    def feedforward(self, inputs):\n",
    "        total = np.dot(self.weights, inputs) + self.bias\n",
    "        return sigmoid(total)\n",
    "\n",
    "weights = np.array([0,1])    # w1 = 0, w2 = 1\n",
    "bias = 4    # c = 4\n",
    "n = Neuron(weights, bias)\n",
    "x = np.array([2,3])   # x = 2, y = 3\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, x, y):\n",
    "        self.input    = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y        = y\n",
    "        self.output   = np.zeros(y.shape)\n",
    "    def feedforward(self):\n",
    "        self.layer1 = sigmoid(np.dot(self.input, self.weights1))\n",
    "        self.output = sigmoid(np.dot(self.layer1, self.weights2))\n",
    "class OurNeuralNetwork:\n",
    "    \"\"\"\n",
    "    –¥–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
    "        ‚àí —Ç—Ä–∏ –≤—Ö–æ–¥–∞ (ùë•1,ùë•2,ùë•3);\n",
    "        ‚àí —Ç—Ä–∏ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (‚Ñé1,‚Ñé2,‚Ñé3);\n",
    "        ‚àí –≤—ã—Ö–æ–¥ (ùëú1).\n",
    "    –Ω–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
    "        ‚àí ùë§ = [0.5,0.5,0.5]\n",
    "        ‚àí ùëè = 0\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        weights = np.array([0.5,0.5,0.5])\n",
    "        bias = 0\n",
    "        # –∫–ª–∞—Å—Å Neuron –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.h3 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        out_h3 = self.h3.feedforward(x)\n",
    "        # –≤—Ö–æ–¥—ã –¥–ª—è –æ1 - —ç—Ç–æ –≤—ã—Ö–æ–¥—ã h1 –∏ h2 –∏ h3\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2, out_h3]))\n",
    "        return out_o1\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2,3,4])\n",
    "print(network.feedforward(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8671195555587996\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class OurNeuralNetwork:\n",
    "    \"\"\"\n",
    "    –¥–∞–Ω–Ω—ã–µ –Ω–µ–π—Ä–æ—Å–µ—Ç–∏:\n",
    "        - –¥–≤–∞ –≤—Ö–æ–¥–∞\n",
    "        - –¥–≤–∞ –Ω–µ–π—Ä–æ–Ω–∞ –≤ —Å–∫—Ä—ã—Ç—ã—Ö —Å–ª–æ—è—Ö (h1, h2)\n",
    "        - –≤—ã—Ö–æ–¥ (–æ1, o2)\n",
    "    –Ω–µ–π—Ä–æ–Ω—ã –∏–º–µ—é—Ç –∏–¥–µ–Ω—Ç–∏—á–Ω—ã–µ –≤–µ—Å–∞ –∏ –ø–æ—Ä–æ–≥–∏:\n",
    "        - w = [1,0]\n",
    "        - b = 1\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        weights = np.array([1,0])\n",
    "        bias = 1\n",
    "        # –∫–ª–∞—Å—Å Neuron –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞\n",
    "        self.h1 = Neuron(weights, bias)\n",
    "        self.h2 = Neuron(weights, bias)\n",
    "        self.o1 = Neuron(weights, bias)\n",
    "        self.o2 = Neuron(weights, bias)\n",
    "        self.o = Neuron(weights, bias)\n",
    "    def feedforward(self, x):\n",
    "        out_h1 = self.h1.feedforward(x)\n",
    "        out_h2 = self.h2.feedforward(x)\n",
    "        # –≤—Ö–æ–¥—ã –¥–ª—è –æ1 - —ç—Ç–æ –≤—ã—Ö–æ–¥—ã h1 –∏ h2\n",
    "        out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))\n",
    "        out_o2 = self.o2.feedforward(np.array([out_h2, out_h1]))\n",
    "        out_o = self.o.feedforward(np.array([out_o2, out_o1]))\n",
    "        return out_o\n",
    "\n",
    "network = OurNeuralNetwork()\n",
    "x = np.array([2,3])\n",
    "print(network.feedforward(x))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "–†–µ–∞–ª–∏–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã –Ω–µ–π—Ä–æ–Ω–Ω—ã—Ö —Å–µ—Ç–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –¥—Ä—É–≥–∏—Ö —Ñ—É–Ω–∫—Ü–∏–π –∞–∫—Ç–∏–≤–∞—Ü–∏–∏.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cmath import tanh\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input    = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y        = y\n",
    "        self.output   = np.zeros(y.shape)\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        return 1 - tanh(x) * tanh(x)\n",
    "    def feedforward(self):\n",
    "        self.layer1 = tanh(np.dot(self.input, self.weights1))\n",
    "        self.output = tanh(np.dot(self.layer1, self.weights2))\n",
    "    def backprop(self):\n",
    "        # –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ —Ü–µ–ø–∏ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤–µ—Å—É2 –∏ –≤–µ—Å—É1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * self.tanh_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T, (np.dot(2*(self.y - self.output) * self.tanh_derivative(self.output), self.weights2.T) * self.tanh_derivative(self.layer1)))\n",
    "        # –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π (–Ω–∞–∫–ª–æ–Ω–∞) —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cmath import tanh\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, x, y):\n",
    "        self.input    = x\n",
    "        self.weights1 = np.random.rand(self.input.shape[1],4)\n",
    "        self.weights2 = np.random.rand(4,1)\n",
    "        self.y        = y\n",
    "        self.output   = np.zeros(y.shape)\n",
    "\n",
    "    def ReLU_derivative(self, x):\n",
    "        if x >= 0:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    def feedforward(self):\n",
    "        self.layer1 = max(0, np.dot(self.input, self.weights1))\n",
    "        self.output = max(0, np.dot(self.layer1, self.weights2))\n",
    "    def backprop(self):\n",
    "        # –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –ø—Ä–∞–≤–∏–ª–∞ —Ü–µ–ø–∏ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å –ø–æ –≤–µ—Å—É2 –∏ –≤–µ—Å—É1\n",
    "        d_weights2 = np.dot(self.layer1.T, (2*(self.y - self.output) * self.ReLU_derivative(self.output)))\n",
    "        d_weights1 = np.dot(self.input.T, (np.dot(2*(self.y - self.output) * self.ReLU_derivative(self.output), self.weights2.T) * self.ReLU_derivative(self.layer1)))\n",
    "        # –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–Ω–æ–π (–Ω–∞–∫–ª–æ–Ω–∞) —Ñ—É–Ω–∫—Ü–∏–∏ –ø–æ—Ç–µ—Ä—å\n",
    "        self.weights1 += d_weights1\n",
    "        self.weights2 += d_weights2"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "–ó–∞–¥–∞–Ω–∏–µ\n",
    "\n",
    "–ó–∞–¥–∞—á–∞: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–ª–∞—Å—Å—ã MLPClassified –∏ MLPRegressor –¥–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –∏ —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –ø—Ä–æ–∏–∑–≤–æ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞. –ü—Ä–æ–≤–µ–¥–∏—Ç–µ –∞–Ω–∞–ª–∏–∑ –∞—Ç—Ä–∏–±—É—Ç—ã, –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π. –î–ª—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ –º–æ–∂–µ—Ç–µ –≤–∑—è—Ç—å –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ò—Ä–∏—Å–æ–≤: https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv –∞ –¥–ª—è —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –∑–∞—Ä–∞–±–æ—Ç–Ω–æ–π –ø–ª–∞—Ç—ã –æ—Ç –æ–ø—ã—Ç–∞ —Ä–∞–±–æ—Ç—ã: https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sad/PycharmProjects/pythonProject2/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_3554/3458819431.py:22: FutureWarning: The behavior of `series[i:j]` with an integer-dtype index is deprecated. In a future version, this will be treated as *label-based* indexing, consistent with e.g. `series[i]` lookups. To retain the old behavior, use `series.iloc[i:j]`. To get the future behavior, use `series.loc[i:j]`.\n",
      "  print(y_test[:15])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Setosa' 'Setosa' 'Virginica' 'Versicolor' 'Virginica' 'Versicolor'\n",
      " 'Setosa' 'Setosa' 'Setosa' 'Setosa' 'Setosa' 'Virginica' 'Versicolor'\n",
      " 'Versicolor' 'Versicolor']\n",
      "19         Setosa\n",
      "49         Setosa\n",
      "83     Versicolor\n",
      "93     Versicolor\n",
      "113     Virginica\n",
      "69     Versicolor\n",
      "31         Setosa\n",
      "37         Setosa\n",
      "25         Setosa\n",
      "10         Setosa\n",
      "39         Setosa\n",
      "107     Virginica\n",
      "51     Versicolor\n",
      "94     Versicolor\n",
      "50     Versicolor\n",
      "Name: variety, dtype: object\n",
      "Test Accurancy: 0.933\n",
      "Training Accurancy: 0.975\n",
      "Loss:  0.32402012120284457\n",
      "Number of coefs:  2\n",
      "Number of intercepts:  2\n",
      "Number of iterations for which estimator ran:  200\n",
      "Name of output layer activation function:  softmax\n",
      "(24, 1) (6, 1) (24,) (6,)\n",
      "[20.26234628 55.2781752  18.82135812 50.48274487 20.26234628 50.9622879 ]\n",
      "[ 54445. 121872.  56642. 116969.  64445. 112635.]\n",
      "Test R^2: -8.796\n",
      "Training R^2: -8.261\n",
      "Loss:  2988058032.1601596\n",
      "Number of coefs:  2\n",
      "Number of intercepts:  2\n",
      "Number of iterations for which estimator ran:  200\n",
      "Name of output layer activation function:  identity\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sad/PycharmProjects/pythonProject2/venv/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:702: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "url = r'https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv'\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.iloc[:,:-1],    # –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π - –≤ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    data.iloc[:,-1], # –ø–æ—Å–ª–µ–¥–Ω—é—é –≤ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–∫–ª–∞—Å—Å)\n",
    "    test_size = 0.20 # —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ 20%\n",
    ")\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp_classifier = MLPClassifier(random_state=123)\n",
    "mlp_classifier.fit(X_train, y_train)\n",
    "Y_preds = mlp_classifier.predict(X_test)\n",
    "\n",
    "print(Y_preds[:15])\n",
    "print(y_test[:15])\n",
    "## –º–º—Ç–µ–¥–æ Score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "print('Test Accurancy: %.3f'%mlp_classifier.score(X_test, y_test))\n",
    "print('Training Accurancy: %.3f'%mlp_classifier.score(X_train, y_train))\n",
    "print(\"Loss: \", mlp_classifier.loss_)\n",
    "print(\"Number of coefs: \", len(mlp_classifier.coefs_))\n",
    "print(\"Number of intercepts: \", len(mlp_classifier.intercepts_))\n",
    "print(\"Number of iterations for which estimator ran: \", mlp_classifier.n_iter_)\n",
    "print(\"Name of output layer activation function: \", mlp_classifier.out_activation_)\n",
    "url = r'https://raw.githubusercontent.com/AnnaShestova/salary-years-simple-linear-regression/master/Salary_Data.csv'\n",
    "\n",
    "data = pd.read_csv(url)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    data.iloc[:,:-1].values,    # –≤—Å–µ –∫–æ–ª–æ–Ω–∫–∏ –∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–µ–π - –≤ –ø—Ä–∏–∑–Ω–∞–∫–∏\n",
    "    data.iloc[:,-1].values, # –ø–æ—Å–ª–µ–¥–Ω—é—é –≤ —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é (–∫–ª–∞—Å—Å)\n",
    "    test_size = 0.20, # —Ä–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏ 20%\n",
    "    random_state=123\n",
    ")\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlp_regressor = MLPRegressor(random_state=123)\n",
    "mlp_regressor.fit(X_train, y_train)\n",
    "Y_preds = mlp_regressor.predict(X_test)\n",
    "\n",
    "print(Y_preds[:10])\n",
    "print(y_test[:10])\n",
    "## –º–º—Ç–µ–¥–æ Score –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Ç–æ—á–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏\n",
    "print('Test R^2: %.3f'%mlp_regressor.score(X_test, y_test))\n",
    "print('Training R^2: %.3f'%mlp_regressor.score(X_train, y_train))\n",
    "print(\"Loss: \", mlp_regressor.loss_)\n",
    "print(\"Number of coefs: \", len(mlp_regressor.coefs_))\n",
    "print(\"Number of intercepts: \", len(mlp_regressor.intercepts_))\n",
    "print(\"Number of iterations for which estimator ran: \", mlp_regressor.n_iter_)\n",
    "print(\"Name of output layer activation function: \", mlp_regressor.out_activation_)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
